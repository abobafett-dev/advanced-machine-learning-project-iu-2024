{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. imports",
   "id": "527dc09a6e801a54"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-21T17:49:25.896071Z",
     "start_time": "2024-04-21T17:49:25.890951Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. function convert str of text to list when we upload data",
   "id": "34b767dc4b0973c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:49:25.998216Z",
     "start_time": "2024-04-21T17:49:25.993729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def str_text_to_list(value):\n",
    "    list_values = value.strip('[]').split(', ')\n",
    "    cleaned_list_values = [item[1:-1] for item in list_values]\n",
    "    return cleaned_list_values"
   ],
   "id": "ead4446463e2d88b",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. take labels from data",
   "id": "5cc1c934a346c2d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:49:31.949103Z",
     "start_time": "2024-04-21T17:49:25.999217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_wsd_1 = pd.read_csv(\"../datasets/tonetags_wsd_1.csv\", index_col=0, converters={\"text\": str_text_to_list})\n",
    "\n",
    "labels = dataset_wsd_1.tags.unique().tolist()\n",
    "\n",
    "del dataset_wsd_1\n",
    "gc.collect()"
   ],
   "id": "f81d39e6e265ead0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. create vocab and embedding by some pretrained embedding",
   "id": "731078c2a96d793c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:49:33.187356Z",
     "start_time": "2024-04-21T17:49:31.950095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "glove_twitter_27B = torchtext.vocab.GloVe(name='twitter.27B', dim=50)\n",
    "\n",
    "vocab = glove_twitter_27B.stoi\n",
    "vocab[\"<unk>\"] = len(vocab)\n",
    "vocab[\"<pad>\"] = len(vocab)\n",
    "\n",
    "embedding_vector = glove_twitter_27B.vectors.numpy()\n",
    "embedding_vector = np.append(embedding_vector, np.zeros(50)).reshape(-1, 50)  # vector for unknown value in vocab\n",
    "embedding_vector = np.append(embedding_vector, np.ones(50)).reshape(-1, 50)  # vector for padding value in vocab\n",
    "\n",
    "embedding_tensor = torch.tensor(embedding_vector, dtype=torch.float)\n",
    "\n",
    "nn.Embedding.from_pretrained(embedding_tensor, freeze=True)"
   ],
   "id": "1959e13edacd772",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(1193516, 50)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. create class of datasets",
   "id": "babd30201b37dcde"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:49:33.217645Z",
     "start_time": "2024-04-21T17:49:33.188357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_length = 4096\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.data = []\n",
    "        for sentence in dataset.text:\n",
    "            if len(sentence) > max_length:\n",
    "                continue\n",
    "            sentence_ids = []\n",
    "            for token in sentence:\n",
    "                try:\n",
    "                    sentence_ids.append(vocab[token])\n",
    "                except KeyError:\n",
    "                    sentence_ids.append(vocab[\"<unk>\"])\n",
    "            self.data.append(sentence_ids)\n",
    "        self.labels = dataset.tags\n",
    "\n",
    "        self.context = None\n",
    "\n",
    "        if 'context' in dataset.columns:\n",
    "            self.context = dataset.context\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.context is None:\n",
    "            return self.data[idx], torch.tensor(self.labels.iloc[idx])\n",
    "        else:\n",
    "            return self.data[idx], torch.tensor(self.labels.iloc[idx]), self.context.iloc[idx]"
   ],
   "id": "99ed60e753e30ad6",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6. Dataloader part",
   "id": "6863c68edfb153e2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:49:33.229894Z",
     "start_time": "2024-04-21T17:49:33.219643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "\n",
    "def collate_fn(batch):\n",
    "    data_ids = []\n",
    "    labels = []\n",
    "    contexts = []\n",
    "\n",
    "    for dat in batch:\n",
    "\n",
    "        data_ids.append(dat[0])\n",
    "        labels.append(dat[1])\n",
    "\n",
    "        if len(dat) >= 3:\n",
    "            contexts.append(dat[2])\n",
    "\n",
    "    for i in range(len(data_ids)):\n",
    "        while len(data_ids[i]) < max_length:\n",
    "            data_ids[i].append(vocab[\"<pad>\"])\n",
    "\n",
    "    return torch.tensor(data_ids), torch.tensor(contexts), torch.tensor(labels)"
   ],
   "id": "9e215a3205c90589",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 7. clean_corrected_text_wsd_2 dataset (from tonetags_dataset_tumblr_clean with glove.twitter.27b.50d)",
   "id": "4251b640a6bb6b89"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7.1. upload dataset",
   "id": "a7cdf9ef60b757fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:49:37.017876Z",
     "start_time": "2024-04-21T17:49:33.230894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def str_context_to_list(value):\n",
    "    list_values = value.strip('[]').split(' ')\n",
    "    cleaned_list_values = []\n",
    "    for item in list_values:\n",
    "        item.strip()\n",
    "        # if '\\n' in item:\n",
    "        #     item.replace('\\n', '')\n",
    "        if item != \"\":\n",
    "            cleaned_list_values.append(float(item))\n",
    "    return cleaned_list_values\n",
    "\n",
    "\n",
    "dataset_clean_corrected_wsd_2 = pd.read_csv(\"../datasets/tonetags_dataset_tumblr_clean_corrected_text_wsd_2.csv\",\n",
    "                                            index_col=0, converters={\"text\": str_text_to_list})\n",
    "\n",
    "dataset_clean_corrected_wsd_2 = dataset_clean_corrected_wsd_2.dropna()\n",
    "dataset_clean_corrected_wsd_2.context = dataset_clean_corrected_wsd_2.context.apply(str_context_to_list)"
   ],
   "id": "d6b450172a852a1c",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7.2. encode labels",
   "id": "66870e64cd73d3dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:49:37.046705Z",
     "start_time": "2024-04-21T17:49:37.018875Z"
    }
   },
   "cell_type": "code",
   "source": "dataset_clean_corrected_wsd_2.tags = dataset_clean_corrected_wsd_2.tags.apply(labels.index)",
   "id": "ee79d9702b33ec27",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7.3. split",
   "id": "922903289e1d01dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:49:37.084260Z",
     "start_time": "2024-04-21T17:49:37.047705Z"
    }
   },
   "cell_type": "code",
   "source": "train_clean_corrected_wsd_2, test_clean_corrected_wsd_2 = train_test_split(dataset_clean_corrected_wsd_2, stratify=dataset_clean_corrected_wsd_2['tags'], test_size=0.2, random_state=42)",
   "id": "355123ed40fc41d6",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7.4. create datasets",
   "id": "350b4f599fed2d34"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:49:38.709540Z",
     "start_time": "2024-04-21T17:49:37.085258Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset_clean_corrected_wsd_2, test_dataset_clean_corrected_wsd_2 = myDataset(train_clean_corrected_wsd_2), myDataset(test_clean_corrected_wsd_2)",
   "id": "cdfb6f5d0965cfd2",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7.5. create dataloaders",
   "id": "fa18eb22aaa10d57"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:49:38.714238Z",
     "start_time": "2024-04-21T17:49:38.710516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataloader_clean_corrected_wsd_2 = DataLoader(train_dataset_clean_corrected_wsd_2, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "\n",
    "test_dataloader_clean_corrected_wsd_2 = DataLoader(test_dataset_clean_corrected_wsd_2, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, drop_last=True)"
   ],
   "id": "1a5635696c16875f",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:49:38.721929Z",
     "start_time": "2024-04-21T17:49:38.716232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ToneTagsLSTM_wsd_2(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding, hidden_dim, context_dim, output_size, num_layers, dropout):\n",
    "        super(ToneTagsLSTM_wsd_2, self).__init__()\n",
    "\n",
    "        # output_size = 19\n",
    "\n",
    "        self.embedding = embedding\n",
    "\n",
    "        self.lstm = nn.LSTM(self.embedding.embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=True, dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim * max_length * 2 + context_dim, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, output_size)\n",
    "        # self.out = nn.Softmax(output_size, dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, tokens, contexts):\n",
    "\n",
    "        embedded = self.embedding(tokens)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        lstm_out = torch.cat((output.reshape(batch_size, -1), contexts), dim=1)\n",
    "\n",
    "        fc1_out = self.fc1(lstm_out)\n",
    "\n",
    "\n",
    "        fc2_out = self.fc2(fc1_out)\n",
    "        out = self.fc3(fc2_out)\n",
    "        # out = self.out(fc3_out)\n",
    "\n",
    "        return out"
   ],
   "id": "60a77aa04a579f2f",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7.6. parameters for clean_corrected_wsd_2",
   "id": "cd2be1ba729f35d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:49:38.727843Z",
     "start_time": "2024-04-21T17:49:38.722929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train_dataloader_clean_corrected_wsd_2\n",
    "# test_dataloader_clean_corrected_wsd_2\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding = nn.Embedding.from_pretrained(embedding_tensor, freeze=True)\n",
    "hidden_dim = 30 # 30\n",
    "context_dim = 50 # 50\n",
    "output_size = len(labels) # 19\n",
    "num_layers = 4\n",
    "dropout = 0.2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr = 5e-4\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "id": "5868bd0df8d29f69",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7.7. create instance of model and optimizer to clean_corrected_wsd_2 data",
   "id": "5b363f9dd175b50b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:49:40.518938Z",
     "start_time": "2024-04-21T17:49:38.729840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train_dataloader_clean_corrected_wsd_2\n",
    "# test_dataloader_clean_corrected_wsd_2\n",
    "\n",
    "model_clean_corrected_wsd_2 = ToneTagsLSTM_wsd_2(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding=embedding,\n",
    "    hidden_dim=hidden_dim,\n",
    "    context_dim=context_dim,\n",
    "    output_size=output_size,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "optimizer_clean_corrected_wsd_2 = optim.Adam(model_clean_corrected_wsd_2.parameters(), lr=lr)"
   ],
   "id": "232d148b018a380",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7.8. train and test to clean_corrected_wsd_2",
   "id": "f0b77457aeb94f88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T18:51:23.145016Z",
     "start_time": "2024-04-21T17:49:40.519925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "for ep in range(1, 11):\n",
    "\n",
    "    model_clean_corrected_wsd_2.train()\n",
    "    epoch_losses_train = []\n",
    "\n",
    "\n",
    "    for tokens, contexts, tags in tqdm(train_dataloader_clean_corrected_wsd_2, desc=f\"Epoch {ep} training...\"):\n",
    "        optimizer_clean_corrected_wsd_2.zero_grad()\n",
    "\n",
    "        tokens = tokens.to(device)\n",
    "        contexts = contexts.to(device)\n",
    "        tags = tags.to(device)\n",
    "\n",
    "        predictions = model_clean_corrected_wsd_2(tokens, contexts)\n",
    "\n",
    "        loss = criterion(predictions, tags)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer_clean_corrected_wsd_2.step()\n",
    "\n",
    "        epoch_losses_train.append(loss.item())\n",
    "\n",
    "    del tokens\n",
    "    del contexts\n",
    "    del tags\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f'[Train Epoch {ep}] Loss: {np.mean(epoch_losses_train)}')"
   ],
   "id": "7f98519c92c72bb8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 training...: 100%|██████████| 2084/2084 [06:03<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train Epoch 1] Loss: 2.7893472200620657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 training...: 100%|██████████| 2084/2084 [05:58<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train Epoch 2] Loss: 2.4650996071134554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 training...: 100%|██████████| 2084/2084 [06:05<00:00,  5.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train Epoch 3] Loss: 2.4317184411518404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 training...: 100%|██████████| 2084/2084 [06:11<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train Epoch 4] Loss: 2.4173328384556836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 training...: 100%|██████████| 2084/2084 [06:12<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train Epoch 5] Loss: 2.4133007140855187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 training...: 100%|██████████| 2084/2084 [06:13<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train Epoch 6] Loss: 2.408822317670266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 training...: 100%|██████████| 2084/2084 [06:10<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train Epoch 7] Loss: 2.4047703767005864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 training...: 100%|██████████| 2084/2084 [06:12<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train Epoch 8] Loss: 2.400300346043197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 training...: 100%|██████████| 2084/2084 [06:13<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train Epoch 9] Loss: 2.3878835375043574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 training...: 100%|██████████| 2084/2084 [06:13<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train Epoch 10] Loss: 2.357204598851945\n",
      "CPU times: total: 1h 1min 43s\n",
      "Wall time: 1h 1min 42s\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8.8. delete some data",
   "id": "fe679df9bd98833"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T18:51:34.887106Z",
     "start_time": "2024-04-21T18:51:23.146017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "del train_dataloader_clean_corrected_wsd_2\n",
    "del test_dataloader_clean_corrected_wsd_2\n",
    "del train_dataset_clean_corrected_wsd_2\n",
    "del test_dataset_clean_corrected_wsd_2\n",
    "del train_clean_corrected_wsd_2\n",
    "del test_clean_corrected_wsd_2\n",
    "del dataset_clean_corrected_wsd_2\n",
    "del optimizer_clean_corrected_wsd_2\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "4e995b1e075b58a",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 9. export and import model",
   "id": "9bb90045724a5de1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T18:51:50.900304Z",
     "start_time": "2024-04-21T18:51:34.888108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.save(model_clean_corrected_wsd_2, \"../results/models/lstm_model.pt\")\n",
    "\n",
    "model_clean_corrected_wsd_2 = torch.load(\"../results/models/lstm_model.pt\")\n",
    "model_clean_corrected_wsd_2.eval()"
   ],
   "id": "25fd6ddbaf7aaec2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToneTagsLSTM_wsd_2(\n",
       "  (embedding): Embedding(1193516, 50)\n",
       "  (lstm): LSTM(50, 30, num_layers=4, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (fc1): Linear(in_features=245810, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (fc3): Linear(in_features=512, out_features=19, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
