{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd419072-f154-40ef-a1b6-ed1abe72486f",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-08T18:43:55.649330Z",
     "start_time": "2024-04-08T18:43:52.037614Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy import average\n",
    "from numpy.linalg import norm\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('stopwords')\n",
    "en_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3f2f8d0-25e1-4980-a67a-de38f0a49a87",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-08T18:43:55.655088Z",
     "start_time": "2024-04-08T18:43:55.650336Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply(tokens):\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    \n",
    "    new_tokens = []\n",
    "    for t in tokens:\n",
    "        if t not in en_stopwords:\n",
    "            new_tokens.append(t)\n",
    "    tokens = new_tokens\n",
    "    \n",
    "    new_sentence = []\n",
    "    for w in tokens:\n",
    "        res = lesk(context_sentence=tokens, ambiguous_word=w)\n",
    "        if res is None:\n",
    "            new_sentence.append(w)\n",
    "            continue\n",
    "        else:\n",
    "            tok = nltk.word_tokenize(res.definition())\n",
    "            new_tokens = []\n",
    "            for t in tok:\n",
    "                if t not in en_stopwords:\n",
    "                    new_tokens.append(t)\n",
    "            tok = new_tokens\n",
    "            new_sentence = new_sentence + tok\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc68eb71-4871-4c52-b65e-f28a4d96b4dd",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-08T18:43:55.659276Z",
     "start_time": "2024-04-08T18:43:55.656094Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = ['weve', 'talked', 'gifted', 'child', 'struggling', 'adult', 'pipeline', ',', 'also', 'old', 'soul', 'immature-seeming', 'adult', 'pipeline', '?', '(', 'spectrum', '?', ')']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a93c670-13aa-40c2-9f75-0f8695ee0e8e",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-08T18:43:56.515571Z",
     "start_time": "2024-04-08T18:43:55.660282Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['weve',\n 'express',\n 'speech',\n 'endowed',\n 'talent',\n 'talents',\n 'member',\n 'clan',\n 'tribe',\n 'engaged',\n 'struggle',\n 'overcome',\n 'especially',\n 'poverty',\n 'obscurity',\n 'designed',\n 'arouse',\n 'lust',\n 'pipe',\n 'used',\n 'transport',\n 'liquids',\n 'gases',\n ',',\n 'addition',\n 'preceding',\n 'something',\n 'else',\n 'time',\n 'order',\n 'secular',\n 'form',\n 'gospel',\n 'major',\n 'Black',\n 'musical',\n 'genre',\n '1960s',\n '1970s',\n 'immature-seeming',\n 'designed',\n 'arouse',\n 'lust',\n 'pipe',\n 'used',\n 'transport',\n 'liquids',\n 'gases',\n '?',\n '(',\n 'broad',\n 'range',\n 'related',\n 'objects',\n 'values',\n 'qualities',\n 'ideas',\n 'activities',\n '?',\n ')']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "985736d4-b507-4944-bea1-3b40959ae184",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:43:56.520335Z",
     "start_time": "2024-04-08T18:43:56.516576Z"
    }
   },
   "outputs": [],
   "source": [
    "#GloVe source:\n",
    "# https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f448029-478c-4973-bba3-04eb105dfb61",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-08T18:43:56.532368Z",
     "start_time": "2024-04-08T18:43:56.522362Z"
    }
   },
   "outputs": [],
   "source": [
    "GLOVE_DEF_PATH = os.path.join(os.getcwd(),'../datasets/glove/glove.6B/glove.6B.50d.txt')\n",
    "def load_glove_vectors(glove_file = GLOVE_DEF_PATH):\n",
    "    \n",
    "    f = open(glove_file, 'r', encoding=\"utf-8\")\n",
    "    vectors = {}\n",
    "    for line in f:\n",
    "        split_line = line.split()\n",
    "        word = split_line[0]\n",
    "        embedding = np.array([float(val) for val in split_line[1:]])\n",
    "        vectors[word] = embedding\n",
    "    f.close()\n",
    "    return vectors\n",
    "\n",
    "\n",
    "cosine_sim_threshold = 0.05\n",
    "score_margin_threshold = 0.1\n",
    "\n",
    "\n",
    "def get_valid_pos_tag(tag):\n",
    "    if tag.startswith('J') or tag.startswith('V') or tag.startswith('N') or tag.startswith('R'):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_word_sense_vectors(candidate):\n",
    "    vectors = {}\n",
    "    try:\n",
    "        candidate_vec = glove[candidate]\n",
    "    except Exception:\n",
    "        return None\n",
    "    for sense in wn.lemmas(candidate):\n",
    "        gloss = [sense.synset().definition()]\n",
    "        gloss.extend(sense.synset().examples())\n",
    "        word_vectors = []\n",
    "        for sentence in gloss:\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "            pos_tags = nltk.pos_tag(tokens)\n",
    "            for gloss_pos, tag in pos_tags:\n",
    "                if get_valid_pos_tag(tag):\n",
    "                    try:\n",
    "                        gloss_word_vec = glove[gloss_pos]\n",
    "                    except Exception:\n",
    "                        # print(gloss_pos, \"not found in glove\")\n",
    "                        continue\n",
    "                    cos_sim = dot(gloss_word_vec, candidate_vec) / (norm(gloss_word_vec) * norm(candidate_vec))\n",
    "                    if cos_sim > cosine_sim_threshold:\n",
    "                        word_vectors.append(gloss_word_vec)\n",
    "        if len(word_vectors) == 0:\n",
    "            continue\n",
    "        sense_vector = average(word_vectors, 0)\n",
    "        vectors[sense] = sense_vector\n",
    "    return vectors\n",
    "\n",
    "\n",
    "def disambiguate_word_sense(word, context_vector):\n",
    "    vectors = sense_vectors_collection[word]\n",
    "    if len(vectors) == 0:\n",
    "        return [None, 0.0]\n",
    "    cos_sims = {}\n",
    "    for sense, sense_vector in vectors.items():\n",
    "        cos_sim = dot(context_vector, sense_vector) / (norm(context_vector) * norm(sense_vector))\n",
    "        cos_sims[sense] = cos_sim\n",
    "    sorted_list = sorted(cos_sims.items(), key=lambda x: x[1])\n",
    "    if len(sorted_list) == 0:\n",
    "        return [None, 0.0]\n",
    "    most_similar_pair = sorted_list.pop()\n",
    "    disambiguated_sense = most_similar_pair[0]\n",
    "    cos_sim_second_most_similar_sense = 0\n",
    "    if len(sorted_list) > 0:\n",
    "        cos_sim_second_most_similar_sense = sorted_list.pop()[1]\n",
    "    score_margin = most_similar_pair[1] - cos_sim_second_most_similar_sense\n",
    "    # we return the disambiguated sense AND the cosine score margin between the two most similar senses.\n",
    "    return [disambiguated_sense, score_margin]\n",
    "\n",
    "\n",
    "sense_vectors_collection = {}\n",
    "\n",
    "def run_algorithm(tokens_input):\n",
    "    global sense_vectors_collection\n",
    "    sorted_sense_vectors_collection = {}\n",
    "    pos_tags_input = nltk.pos_tag(tokens_input)\n",
    "    \n",
    "    pos = []\n",
    "    pos_vectors = {}\n",
    "    for word, pos_tag in pos_tags_input:\n",
    "        if get_valid_pos_tag(pos_tag):\n",
    "            try:\n",
    "                pos_vectors[word] = glove[word]\n",
    "                pos.append(word)\n",
    "            except Exception:\n",
    "                pass\n",
    "                \n",
    "    # Sense vectors init\n",
    "    for p in pos:\n",
    "        sense_vectors = get_word_sense_vectors(p)\n",
    "        if sense_vectors is None:\n",
    "            continue\n",
    "        sense_vectors_collection[p] = sense_vectors\n",
    "        sorted_sense_vectors_collection[p] = len(sense_vectors)\n",
    "    \n",
    "    # S2C sorting for content word\n",
    "    sorted_sense_vectors_collection = sorted(sorted_sense_vectors_collection.items(), key=lambda x: x[1])\n",
    "    \n",
    "    # Context vector initialization\n",
    "    context_vec = average(list(pos_vectors.values()), 0)\n",
    "    \n",
    "    for w, _ in sorted_sense_vectors_collection:\n",
    "        disambiguation_results = disambiguate_word_sense(w, context_vec)\n",
    "        disambiguated_sense = disambiguation_results[0]\n",
    "        if disambiguated_sense is None:\n",
    "            continue\n",
    "        score_margin = disambiguation_results[1]\n",
    "        if score_margin > score_margin_threshold:\n",
    "            pos_vectors[w] = sense_vectors_collection[w][disambiguated_sense]\n",
    "            context_vec = average(list(pos_vectors.values()), 0)\n",
    "    sense_vectors_collection.clear()\n",
    "    return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deb7cc3d-2825-44a9-8f56-d84c45f52def",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-08T18:44:00.712354Z",
     "start_time": "2024-04-08T18:43:56.533373Z"
    }
   },
   "outputs": [],
   "source": [
    "glove = load_glove_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2173f9a-979c-4b58-9ef8-6888bb3214cc",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-08T18:44:00.814743Z",
     "start_time": "2024-04-08T18:44:00.713418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{Lemma('key.n.01.key'): array([ 0.23398828,  0.06947961, -0.10388128, -0.0099533 ,  0.43481728,\n         0.29401644,  0.20012272, -0.43976511, -0.13210233,  0.05348277,\n         0.24428633,  0.05993198, -0.21467228, -0.06295217, -0.04845044,\n         0.20116533, -0.09243417,  0.01657027,  0.18980996, -0.27478444,\n        -0.02330133, -0.31139944, -0.29876339, -0.07223167,  0.08578778,\n        -1.26486   , -0.35051844, -0.13795044,  0.29510317,  0.24838428,\n         2.94410167, -0.24746111, -0.07286117, -0.72782778,  0.00712588,\n        -0.03365111, -0.01599744,  0.18017395,  0.01516489,  0.35594933,\n        -0.23689944, -0.13363701,  0.18609783,  0.42248206, -0.14563406,\n         0.41616411, -0.05148722,  0.25225161,  0.07375754, -0.07387137]),\n Lemma('key.n.07.Key'): array([-0.01803425,  0.3279965 , -0.5628375 , -0.43857317,  0.40369858,\n         0.27693233, -0.45595677,  0.14854517, -0.40123083,  0.36382783,\n        -0.25543275,  0.24510917, -0.254656  , -0.18743725,  0.547935  ,\n        -0.62914488, -0.07942704, -0.30761997, -0.33955508,  0.27023642,\n         0.34401017,  0.47687783,  0.12042725,  0.01984663,  0.71557042,\n        -1.337465  , -0.78925083, -0.28893658, -0.24758392, -0.09633192,\n         2.28508   , -0.91980908, -0.10685867, -0.298982  , -0.13843567,\n        -0.07669825,  0.34395575, -0.02552342, -0.21421442,  0.15679775,\n         0.29233233,  0.24900125, -0.41338792, -0.3960025 , -0.093075  ,\n         0.03714767, -0.14529992, -0.27268675, -0.146498  , -0.00851   ])}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_sense_vectors('key')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def str_to_list(value):\n",
    "    list_values = value.strip('[]').split(', ')\n",
    "    cleaned_list_values = [item[1:-1] for item in list_values]\n",
    "    return cleaned_list_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T18:44:00.819259Z",
     "start_time": "2024-04-08T18:44:00.815749Z"
    }
   },
   "id": "ae795fbcf247c2fd",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"../datasets/tonetags_dataset_tumblr_clean.csv\", converters={\"text\": str_to_list})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T18:44:01.769954Z",
     "start_time": "2024-04-08T18:44:00.820266Z"
    }
   },
   "id": "93dd12ccf08b8145",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m df2 \u001B[38;5;241m=\u001B[39m df1\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[1;32m----> 2\u001B[0m df2[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m df2[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x:apply(x))\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\series.py:4904\u001B[0m, in \u001B[0;36mSeries.apply\u001B[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[0m\n\u001B[0;32m   4769\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[0;32m   4770\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4771\u001B[0m     func: AggFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4776\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   4777\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[0;32m   4778\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4779\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[0;32m   4780\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4895\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[0;32m   4896\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m   4897\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m SeriesApply(\n\u001B[0;32m   4898\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4899\u001B[0m         func,\n\u001B[0;32m   4900\u001B[0m         convert_dtype\u001B[38;5;241m=\u001B[39mconvert_dtype,\n\u001B[0;32m   4901\u001B[0m         by_row\u001B[38;5;241m=\u001B[39mby_row,\n\u001B[0;32m   4902\u001B[0m         args\u001B[38;5;241m=\u001B[39margs,\n\u001B[0;32m   4903\u001B[0m         kwargs\u001B[38;5;241m=\u001B[39mkwargs,\n\u001B[1;32m-> 4904\u001B[0m     )\u001B[38;5;241m.\u001B[39mapply()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\apply.py:1427\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1424\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_compat()\n\u001B[0;32m   1426\u001B[0m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[1;32m-> 1427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_standard()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\apply.py:1507\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1501\u001B[0m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[0;32m   1504\u001B[0m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[0;32m   1505\u001B[0m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[0;32m   1506\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mdtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1507\u001B[0m mapped \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39m_map_values(\n\u001B[0;32m   1508\u001B[0m     mapper\u001B[38;5;241m=\u001B[39mcurried, na_action\u001B[38;5;241m=\u001B[39maction, convert\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconvert_dtype\n\u001B[0;32m   1509\u001B[0m )\n\u001B[0;32m   1511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[0;32m   1512\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[0;32m   1513\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[0;32m   1514\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[1;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[0;32m    918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[0;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mmap(mapper, na_action\u001B[38;5;241m=\u001B[39mna_action)\n\u001B[1;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m algorithms\u001B[38;5;241m.\u001B[39mmap_array(arr, mapper, na_action\u001B[38;5;241m=\u001B[39mna_action, convert\u001B[38;5;241m=\u001B[39mconvert)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\algorithms.py:1743\u001B[0m, in \u001B[0;36mmap_array\u001B[1;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[0;32m   1741\u001B[0m values \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer(values, mapper, convert\u001B[38;5;241m=\u001B[39mconvert)\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1745\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer_mask(\n\u001B[0;32m   1746\u001B[0m         values, mapper, mask\u001B[38;5;241m=\u001B[39misna(values)\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39muint8), convert\u001B[38;5;241m=\u001B[39mconvert\n\u001B[0;32m   1747\u001B[0m     )\n",
      "File \u001B[1;32mlib.pyx:2972\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "Cell \u001B[1;32mIn[11], line 2\u001B[0m, in \u001B[0;36m<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m      1\u001B[0m df2 \u001B[38;5;241m=\u001B[39m df1\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[1;32m----> 2\u001B[0m df2[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m df2[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x:apply(x))\n",
      "Cell \u001B[1;32mIn[2], line 12\u001B[0m, in \u001B[0;36mapply\u001B[1;34m(tokens)\u001B[0m\n\u001B[0;32m     10\u001B[0m new_sentence \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m tokens:\n\u001B[1;32m---> 12\u001B[0m     res \u001B[38;5;241m=\u001B[39m lesk(context_sentence\u001B[38;5;241m=\u001B[39mtokens, ambiguous_word\u001B[38;5;241m=\u001B[39mw)\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m res \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     14\u001B[0m         new_sentence\u001B[38;5;241m.\u001B[39mappend(w)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\wsd.py:39\u001B[0m, in \u001B[0;36mlesk\u001B[1;34m(context_sentence, ambiguous_word, pos, synsets)\u001B[0m\n\u001B[0;32m     37\u001B[0m context \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(context_sentence)\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m synsets \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 39\u001B[0m     synsets \u001B[38;5;241m=\u001B[39m wordnet\u001B[38;5;241m.\u001B[39msynsets(ambiguous_word)\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pos:\n\u001B[0;32m     42\u001B[0m     synsets \u001B[38;5;241m=\u001B[39m [ss \u001B[38;5;28;01mfor\u001B[39;00m ss \u001B[38;5;129;01min\u001B[39;00m synsets \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(ss\u001B[38;5;241m.\u001B[39mpos()) \u001B[38;5;241m==\u001B[39m pos]\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1743\u001B[0m, in \u001B[0;36mWordNetCorpusReader.synsets\u001B[1;34m(self, lemma, pos, lang, check_exceptions)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlemma_from_key(sense_key)\u001B[38;5;241m.\u001B[39msynset()\n\u001B[0;32m   1739\u001B[0m \u001B[38;5;66;03m#############################################################\u001B[39;00m\n\u001B[0;32m   1740\u001B[0m \u001B[38;5;66;03m# Retrieve synsets and lemmas.\u001B[39;00m\n\u001B[0;32m   1741\u001B[0m \u001B[38;5;66;03m#############################################################\u001B[39;00m\n\u001B[1;32m-> 1743\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msynsets\u001B[39m(\u001B[38;5;28mself\u001B[39m, lemma, pos\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, lang\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meng\u001B[39m\u001B[38;5;124m\"\u001B[39m, check_exceptions\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load all synsets with a given lemma and part of speech tag.\u001B[39;00m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;124;03m    If no pos is specified, all synsets for all parts of speech\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;124;03m    will be loaded.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;124;03m    If lang is specified, all the synsets associated with the lemma name\u001B[39;00m\n\u001B[0;32m   1748\u001B[0m \u001B[38;5;124;03m    of that language will be returned.\u001B[39;00m\n\u001B[0;32m   1749\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m     lemma \u001B[38;5;241m=\u001B[39m lemma\u001B[38;5;241m.\u001B[39mlower()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "df2 = df1.copy()\n",
    "df2[\"text\"] = df2[\"text\"].apply(lambda x:apply(x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T18:09:46.974509Z",
     "start_time": "2024-04-08T18:09:13.553579Z"
    }
   },
   "id": "bde1a97d03199879",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "               tags                                               text\n0  genuine question  [weve, talked, gifted, child, struggling, adul...\n1  genuine question  [Is, concept, straight, girls, religiously, wa...\n2  genuine question  [I, sincerely, curious, ., Has, ever, written,...\n3  genuine question  [Bro, idk, people, look, fucking, good, fake, ...\n4  genuine question  [Advice, beginner, witch, supplies, deal, peri...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tags</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>genuine question</td>\n      <td>[weve, talked, gifted, child, struggling, adul...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>genuine question</td>\n      <td>[Is, concept, straight, girls, religiously, wa...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>genuine question</td>\n      <td>[I, sincerely, curious, ., Has, ever, written,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>genuine question</td>\n      <td>[Bro, idk, people, look, fucking, good, fake, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>genuine question</td>\n      <td>[Advice, beginner, witch, supplies, deal, peri...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T18:44:01.799368Z",
     "start_time": "2024-04-08T18:44:01.771961Z"
    }
   },
   "id": "f9d90169ff5adabf",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df2.to_csv(\"../datasets/tonetags_wsd_1.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-08T18:00:04.663405Z"
    }
   },
   "id": "39b4168b6a2399db",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df3 = df1.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T18:44:01.805855Z",
     "start_time": "2024-04-08T18:44:01.800373Z"
    }
   },
   "id": "19435e284d0eb0db",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/85304 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88ad8f2dc30f4811a0352d954f73a3f8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sasha\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\lib\\function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "C:\\Users\\sasha\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "df3[\"context\"] = df3[\"text\"].progress_apply(lambda x: run_algorithm(x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T02:26:29.057029Z",
     "start_time": "2024-04-08T18:44:01.806860Z"
    }
   },
   "id": "208e7400753721e9",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df3.to_csv(\"../datasets/tonetags_wsd_2.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:41:14.237095Z",
     "start_time": "2024-04-09T03:41:00.765930Z"
    }
   },
   "id": "823353b342ee8826",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f4efbf0a4553b74c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
