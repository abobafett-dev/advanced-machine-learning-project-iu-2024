{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bd419072-f154-40ef-a1b6-ed1abe72486f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/danil-\n",
      "[nltk_data]     pass123/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import  nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "nltk.download('stopwords')\n",
    "en_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c3f2f8d0-25e1-4980-a67a-de38f0a49a87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def apply_to_text(text):\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    return ' '.join([apply(s) for s in sentences])\n",
    "    \n",
    "\n",
    "def apply(sentence):\n",
    "    \n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    \n",
    "    new_tokens = []\n",
    "    for t in tokens:\n",
    "        if t not in en_stopwords:\n",
    "            new_tokens.append(t)\n",
    "    tokens = new_tokens\n",
    "    \n",
    "    new_sentance = ''\n",
    "    for w in tokens:\n",
    "        pos = nltk.pos_tag([w])[0][1]\n",
    "        \n",
    "        res = lesk(context_sentence = tokens, ambiguous_word=w, pos= None)\n",
    "        if res is None:\n",
    "            new_sentance = new_sentance + w + ' '\n",
    "            continue\n",
    "        \n",
    "        new_sentance = new_sentance + res.definition() + ' '\n",
    "\n",
    "    return new_sentance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dc68eb71-4871-4c52-b65e-f28a4d96b4dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentence = 'Word is only a part of sentence.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7a93c670-13aa-40c2-9f75-0f8695ee0e8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a word is a string of bits stored in computer memory the part played by a person in bringing about a result pronounce a sentence on (somebody) in a court of law . '"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a982111f-fb62-44cd-b5e4-4f111d577b79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a word is a string of bits stored in computer memory the part played by a person in bringing about a result pronounce a sentence on (somebody) in a court of law .  pronounce a sentence on (somebody) in a court of law very small the part played by a person in bringing about a result a book prepared for use in schools or colleges '"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_to_text('Word is only a part of sentence. Sentence is only a tiny part of text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985736d4-b507-4944-bea1-3b40959ae184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GloVe source:\n",
    "# https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "641b302b-3855-4634-9aeb-ab728d3bc56f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glove.6B.zip', 'glove.6B', 'archive.zip', 'Sentiment-Analysis-Dataset.zip']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4f448029-478c-4973-bba3-04eb105dfb61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy import average\n",
    "from numpy.linalg import norm\n",
    "import os\n",
    "\n",
    "# from gensim.models import Word2Vec\n",
    "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "GLOVE_DEF_PATH = os.path.join(os.getcwd(),'../datasets/glove/glove.6B/glove.6B.50d.txt')\n",
    "def load_glove_vectors(glove_file = GLOVE_DEF_PATH):\n",
    "    \n",
    "    f = open(glove_file, 'r', encoding=\"utf-8\")\n",
    "    vectors = {}\n",
    "    for line in f:\n",
    "        split_line = line.split()\n",
    "        word = split_line[0]\n",
    "        embedding = np.array([float(val) for val in split_line[1:]])\n",
    "        vectors[word] = embedding\n",
    "    f.close()\n",
    "    return vectors\n",
    "\n",
    "\n",
    "cosine_sim_threshold = 0.05\n",
    "score_margin_threshold = 0.1\n",
    "\n",
    "\n",
    "def get_valid_pos_tag(tag):\n",
    "    if tag.startswith('J') or tag.startswith('V') or tag.startswith('N') or tag.startswith('R'):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_word_sense_vectors(candidate):\n",
    "    vectors = {}\n",
    "    try:\n",
    "        candidate_vec = glove[candidate]\n",
    "    except Exception:\n",
    "        # print(candidate, \"not found in glove\")\n",
    "        return None\n",
    "    for sense in wn.lemmas(candidate):\n",
    "        # if candidate == \"bank\":\n",
    "        # print(\"synonym of \", candidate, \" is \", ss.lemmas()[0].name())\n",
    "        # print(\"key of \", candidate, \" is \", ss.lemmas()[0].key())\n",
    "        gloss = [sense.synset().definition()]\n",
    "        gloss.extend(sense.synset().examples())\n",
    "        word_vectors = []\n",
    "        for sentence in gloss:\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "            pos_tags = nltk.pos_tag(tokens)\n",
    "            for gloss_pos, tag in pos_tags:\n",
    "                if get_valid_pos_tag(tag):\n",
    "                    try:\n",
    "                        gloss_word_vec = glove[gloss_pos]\n",
    "                    except Exception:\n",
    "                        # print(gloss_pos, \"not found in glove\")\n",
    "                        continue\n",
    "                    cos_sim = dot(gloss_word_vec, candidate_vec) / (norm(gloss_word_vec) * norm(candidate_vec))\n",
    "                    if cos_sim > cosine_sim_threshold:\n",
    "                        word_vectors.append(gloss_word_vec)\n",
    "        if len(word_vectors) == 0:\n",
    "            continue\n",
    "        sense_vector = average(word_vectors, 0)\n",
    "        vectors[sense] = sense_vector\n",
    "    return vectors\n",
    "\n",
    "\n",
    "def disambiguate_word_sense(word, context_vector):\n",
    "    vectors = sense_vectors_collection[word]\n",
    "    if len(vectors) == 0:\n",
    "        return [None, 0.0]\n",
    "    cos_sims = {}\n",
    "    for sense, sense_vector in vectors.items():\n",
    "        cos_sim = dot(context_vector, sense_vector) / (norm(context_vector) * norm(sense_vector))\n",
    "        cos_sims[sense] = cos_sim\n",
    "    sorted_list = sorted(cos_sims.items(), key=lambda x: x[1])\n",
    "    if len(sorted_list) == 0:\n",
    "        return [None, 0.0]\n",
    "    most_similar_pair = sorted_list.pop()\n",
    "    disambiguated_sense = most_similar_pair[0]\n",
    "    cos_sim_second_most_similar_sense = 0\n",
    "    if len(sorted_list) > 0:\n",
    "        cos_sim_second_most_similar_sense = sorted_list.pop()[1]\n",
    "    score_margin = most_similar_pair[1] - cos_sim_second_most_similar_sense\n",
    "    # we return the disambiguated sense AND the cosine score margin between the two most similar senses.\n",
    "    return [disambiguated_sense, score_margin]\n",
    "\n",
    "\n",
    "sense_vectors_collection = {}\n",
    "sorted_sense_vectors_collection = {}\n",
    "\n",
    "def run_algorithm(sentence):\n",
    "    global sorted_sense_vectors_collection,sense_vectors_collection\n",
    "    tokens_input = nltk.word_tokenize(sentence)\n",
    "    pos_tags_input = nltk.pos_tag(tokens_input)\n",
    "    \n",
    "    pos = []\n",
    "    pos_vectors = {}\n",
    "    for word, pos_tag in pos_tags_input:\n",
    "        # print(word, \"is tagged as\", pos_tag)\n",
    "        if get_valid_pos_tag(pos_tag):\n",
    "            try:\n",
    "                pos_vectors[word] = glove[word]\n",
    "                pos.append(word)\n",
    "            except Exception:\n",
    "                pass\n",
    "                \n",
    "    # Sense vectors init\n",
    "    for p in pos:\n",
    "        sense_vectors = get_word_sense_vectors(p)\n",
    "        if sense_vectors is None:\n",
    "            continue\n",
    "        sense_vectors_collection[p] = sense_vectors\n",
    "        sorted_sense_vectors_collection[p] = len(sense_vectors)\n",
    "    \n",
    "    # S2C sorting for content word\n",
    "    sorted_sense_vectors_collection = sorted(sorted_sense_vectors_collection.items(), key=lambda x: x[1])\n",
    "    \n",
    "    # Context vector initialization\n",
    "    context_vec = average(list(pos_vectors.values()), 0)\n",
    "    wn_key = \"not found\"\n",
    "    \n",
    "    \n",
    "    for w, _ in sorted_sense_vectors_collection:\n",
    "        disambiguation_results = disambiguate_word_sense(w, context_vec)\n",
    "        disambiguated_sense = disambiguation_results[0]\n",
    "        if disambiguated_sense is None:\n",
    "            continue\n",
    "        # if w == lookup_word:\n",
    "        #     wn_key = disambiguated_sense._key\n",
    "        #     break\n",
    "        score_margin = disambiguation_results[1]\n",
    "        if score_margin > score_margin_threshold:\n",
    "            pos_vectors[w] = sense_vectors_collection[w][disambiguated_sense]\n",
    "            context_vec = average(list(pos_vectors.values()), 0)\n",
    "    # print(pos_vectors.keys())\n",
    "    sense_vectors_collection.clear()\n",
    "    return context_vec\n",
    "\n",
    "# glove = load_glove_vectors('/media/iftekhar/New Volume/Personal/Admission Docs/Germany/RWTH/MI/Lab - AI_Language_Technology/training_nball47634/glove.6B.50d.txt')\n",
    "# glove = load_glove_vectors('/media/iftekhar/New Volume/Personal/Admission Docs/Germany/RWTH/MI/Lab - AI_Language_Technology/deps.words')\n",
    "# glove = load_glove_vectors('/media/iftekhar/New Volume/Personal/Admission Docs/Germany/RWTH/MI/Lab - AI_Language_Technology/bow2.words')\n",
    "# glove = load_glove_vectors('/media/iftekhar/New Volume/Personal/Admission Docs/Germany/RWTH/MI/Lab - AI_Language_Technology/bow5.words')\n",
    "# glove = load_glove_vectors('E:/Code/deps.words/deps.words')\n",
    "# sense_vectors_collection = {}\n",
    "\n",
    "# annotation_results = dict()\n",
    "\n",
    "\n",
    "# def find_wn_key(sentence, lookup_word):\n",
    "#     sorted_sense_vectors_collection = {}\n",
    "#     pos = []\n",
    "#     pos_vectors = {}\n",
    "#     tokens_input = nltk.word_tokenize(sentence)\n",
    "#     pos_tags_input = nltk.pos_tag(tokens_input)\n",
    "#     for word, pos_tag in pos_tags_input:\n",
    "#         # print(word, \"is tagged as\", pos_tag)\n",
    "#         if get_valid_pos_tag(pos_tag):\n",
    "#             try:\n",
    "#                 pos_vectors[word] = glove[word]\n",
    "#                 pos.append(word)\n",
    "#             except Exception:\n",
    "#                 pass\n",
    "#                 # print(pos, \" not found in glove\")\n",
    "#     for p in pos:\n",
    "#         sense_vectors = get_word_sense_vectors(p)\n",
    "#         if sense_vectors is None:\n",
    "#             continue\n",
    "#         sense_vectors_collection[p] = sense_vectors\n",
    "#         sorted_sense_vectors_collection[p] = len(sense_vectors)\n",
    "#     # S2C sorting for content word\n",
    "#     sorted_sense_vectors_collection = sorted(sorted_sense_vectors_collection.items(), key=lambda x: x[1])\n",
    "#     # print(\"sorted by sense count\", sorted_sense_vectors_collection)\n",
    "#     # Context vector initialization\n",
    "#     context_vec = average(list(pos_vectors.values()), 0)\n",
    "#     wn_key = \"not found\"\n",
    "#     for w, _ in sorted_sense_vectors_collection:\n",
    "#         disambiguation_results = disambiguate_word_sense(w, context_vec)\n",
    "#         disambiguated_sense = disambiguation_results[0]\n",
    "#         if disambiguated_sense is None:\n",
    "#             continue\n",
    "#         if w == lookup_word:\n",
    "#             wn_key = disambiguated_sense._key\n",
    "#             break\n",
    "#         score_margin = disambiguation_results[1]\n",
    "#         if score_margin > score_margin_threshold:\n",
    "#             pos_vectors[w] = sense_vectors_collection[w][disambiguated_sense]\n",
    "#             context_vec = average(list(pos_vectors.values()), 0)\n",
    "#     # print(pos_vectors.keys())\n",
    "#     sense_vectors_collection.clear()\n",
    "#     return wn_key\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# output_file = open(\"output_bow2_windows.txt\", \"w\")\n",
    "# results_file = open(\"wsd_results_bow2_windows.txt\", \"w\")\n",
    "# output_file = open(\"output_bow5_windows.txt\", \"w\")\n",
    "# results_file = open(\"wsd_results_bow5_windows.txt\", \"w\")\n",
    "# output_file = open(\"output_glove.6B.50d_windows.txt\", \"w\")\n",
    "# results_file = open(\"wsd_results_glove.6B.50d_windows.txt\", \"w\")\n",
    "# output_file = open(\"output_deps_windows.txt\", \"w\")\n",
    "# results_file = open(\"wsd_results_deps_windows.txt\", \"w\")\n",
    "\n",
    "# load_annotations()\n",
    "\n",
    "# correct_count = 0\n",
    "# invalid_linkup_key_count = 0\n",
    "# total_sentence_count = 0\n",
    "\n",
    "# for dirpath, _, filenames in os.walk(\"E:/Code/hlt2005releasev2/hlt2005releasev2/domainhltGS.tar/Annotated_Sentences\"):\n",
    "#     if len(filenames) == 0:\n",
    "#         continue\n",
    "#     for file in filenames:\n",
    "#         f = open(os.path.join(dirpath, file), 'r', encoding='ISO-8859-1')\n",
    "#         #from itertools import islice\n",
    "\n",
    "#         #for line in islice(f, 1):\n",
    "#         for line in f:\n",
    "#             split_line = line.split('?')\n",
    "#             metadata_array = split_line[0].split(' ')\n",
    "#             linkup_key = metadata_array[0]\n",
    "#             lookup_word = metadata_array[2]\n",
    "#             sentence = split_line[1].split(' ', 2)[2]\n",
    "#             wn_key = find_wn_key(sentence, lookup_word)\n",
    "#             results_file.write(\"|\" + linkup_key + \"|     \" + wn_key + \"\\n\")\n",
    "#             if linkup_key in annotation_results:\n",
    "#                 total_sentence_count += 1\n",
    "#                 wn_keylist = annotation_results[linkup_key]\n",
    "#                 if len(wn_keylist) > 0:\n",
    "#                     most_frequent_wn_key = max(set(wn_keylist), key=wn_keylist.count)\n",
    "#                     if most_frequent_wn_key == wn_key:\n",
    "#                         print(\"correct wsd for\", linkup_key, wn_key)\n",
    "#                         output_file.write(\"correct wsd for \" + linkup_key + \" \" + wn_key + \"\\n\")\n",
    "#                         correct_count += 1\n",
    "#                         print(\"correct\", correct_count, \"| total\", total_sentence_count)\n",
    "#                         output_file.write(\"correct \" + str(correct_count) + \" | total \" + str(total_sentence_count) + \"\\n\")\n",
    "#                     else:\n",
    "#                         print(\"incorrect wsd for\", linkup_key, \"| found\", wn_key, \", correct is\", most_frequent_wn_key)\n",
    "#                         output_file.write(\"incorrect wsd for \" + linkup_key + \" | found \" + wn_key + \", correct is \" + most_frequent_wn_key + \"\\n\")\n",
    "#             else:\n",
    "#                 invalid_linkup_key_count += 1\n",
    "#                 print(\"linkup key\", linkup_key, \"not found in gold standard clean dataset\")\n",
    "#                 output_file.write(\"linkup key \" + linkup_key + \" not found in gold standard clean dataset\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "deb7cc3d-2825-44a9-8f56-d84c45f52def",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "glove = load_glove_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f2173f9a-979c-4b58-9ef8-6888bb3214cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Lemma('key.n.01.key'): array([ 0.23398828,  0.06947961, -0.10388128, -0.0099533 ,  0.43481728,\n",
       "         0.29401644,  0.20012272, -0.43976511, -0.13210233,  0.05348277,\n",
       "         0.24428633,  0.05993198, -0.21467228, -0.06295217, -0.04845044,\n",
       "         0.20116533, -0.09243417,  0.01657027,  0.18980996, -0.27478444,\n",
       "        -0.02330133, -0.31139944, -0.29876339, -0.07223167,  0.08578778,\n",
       "        -1.26486   , -0.35051844, -0.13795044,  0.29510317,  0.24838428,\n",
       "         2.94410167, -0.24746111, -0.07286117, -0.72782778,  0.00712588,\n",
       "        -0.03365111, -0.01599744,  0.18017395,  0.01516489,  0.35594933,\n",
       "        -0.23689944, -0.13363701,  0.18609783,  0.42248206, -0.14563406,\n",
       "         0.41616411, -0.05148722,  0.25225161,  0.07375754, -0.07387137]),\n",
       " Lemma('key.n.07.Key'): array([-0.01803425,  0.3279965 , -0.5628375 , -0.43857317,  0.40369858,\n",
       "         0.27693233, -0.45595677,  0.14854517, -0.40123083,  0.36382783,\n",
       "        -0.25543275,  0.24510917, -0.254656  , -0.18743725,  0.547935  ,\n",
       "        -0.62914488, -0.07942704, -0.30761997, -0.33955508,  0.27023642,\n",
       "         0.34401017,  0.47687783,  0.12042725,  0.01984663,  0.71557042,\n",
       "        -1.337465  , -0.78925083, -0.28893658, -0.24758392, -0.09633192,\n",
       "         2.28508   , -0.91980908, -0.10685867, -0.298982  , -0.13843567,\n",
       "        -0.07669825,  0.34395575, -0.02552342, -0.21421442,  0.15679775,\n",
       "         0.29233233,  0.24900125, -0.41338792, -0.3960025 , -0.093075  ,\n",
       "         0.03714767, -0.14529992, -0.27268675, -0.146498  , -0.00851   ])}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_sense_vectors('key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cf35788c-fa62-412a-8d75-8414fff1d123",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.01247350e-01,  2.89317000e-01, -3.74097300e-01, -2.59285000e-02,\n",
       "        5.85898000e-01,  3.85850650e-01,  9.84151500e-02, -1.18100750e-01,\n",
       "        4.95730000e-04,  8.60915000e-02,  9.70003500e-02,  7.95810000e-02,\n",
       "       -1.64794365e-01, -2.91452500e-01,  5.42528000e-01,  2.19557315e-01,\n",
       "        1.61640350e-01,  2.59238000e-01, -1.19468990e-01,  9.10579500e-02,\n",
       "       -2.25948400e-01,  3.12650000e-02,  8.43702000e-02,  3.12949295e-01,\n",
       "        3.94029000e-02, -2.00003450e+00, -2.61442000e-01,  2.54413450e-01,\n",
       "        1.46159500e-02, -1.27636000e-01,  3.04883350e+00, -6.52302500e-01,\n",
       "       -3.13209700e-01, -4.75266475e-01,  1.84038150e-01, -2.36682401e-01,\n",
       "        3.69803500e-01,  4.26101150e-02,  7.40252000e-02, -1.52490750e-01,\n",
       "       -2.24771500e-01,  2.70650500e-01, -1.83526500e-02,  1.74195150e-01,\n",
       "       -3.27736800e-01,  1.94538500e-02, -2.38214300e-01, -2.28488500e-01,\n",
       "        1.62012500e-01,  2.26826500e-01])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_algorithm(\"Here is sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca92eb-7725-4d98-af24-ffa73b957af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_sense_vectors_collection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
